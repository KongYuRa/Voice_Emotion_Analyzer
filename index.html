# Advanced Voice Emotion Analyzer - English Version

## index.html

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced Voice Emotion Analyzer</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="app-container">
        <!-- Header -->
        <header class="app-header">
            <h1>Advanced Voice Emotion Analyzer</h1>
            <p>Accurate emotion recognition through MFCC, spectral features, and prosodic analysis</p>
        </header>

        <!-- Main Content -->
        <main class="main-content">
            <!-- Input Section -->
            <section class="input-section">
                <div class="card">
                    <div class="card__header">
                        <h3>Voice Input</h3>
                    </div>
                    <div class="card__body">
                        <div class="input-methods">
                            <div class="method-group">
                                <h4>File Upload</h4>
                                <div class="file-upload-area" id="fileUploadArea">
                                    <input type="file" id="audioFileInput" accept="audio/*" class="hidden">
                                    <div class="upload-placeholder">
                                        <span class="upload-icon">ðŸ“</span>
                                        <p>Drag and drop audio file or click to select</p>
                                        <p class="file-info">Supported formats: MP3, WAV, M4A</p>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="method-group">
                                <h4>Live Recording</h4>
                                <div class="recording-controls">
                                    <button id="recordButton" class="btn btn--primary">
                                        <span class="btn-icon">ðŸŽ™ï¸</span>
                                        Start Recording
                                    </button>
                                    <button id="stopRecordButton" class="btn btn--secondary" style="display: none;">
                                        <span class="btn-icon">â¹ï¸</span>
                                        Stop Recording
                                    </button>
                                    <div class="recording-time" id="recordingTime" style="display: none;">
                                        <span class="time-display">00:00</span>
                                        <div class="recording-indicator"></div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Analysis Results -->
            <section class="results-section">
                <div class="card">
                    <div class="card__header">
                        <h3>Analysis Results</h3>
                        <div class="analysis-status" id="analysisStatus">
                            <span class="status-text">Please upload audio or start recording</span>
                        </div>
                    </div>
                    <div class="card__body">
                        <!-- Main Emotion Result -->
                        <div class="emotion-result" id="emotionResult" style="display: none;">
                            <div class="primary-emotion">
                                <div class="emotion-icon" id="emotionIcon">ðŸ˜</div>
                                <div class="emotion-info">
                                    <h3 class="emotion-name" id="emotionName">Neutral</h3>
                                    <div class="confidence-score">
                                        <span>Confidence: </span>
                                        <span id="confidenceScore">0%</span>
                                        <div class="confidence-bar">
                                            <div class="confidence-fill" id="confidenceFill"></div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <!-- Alternative Emotions -->
                            <div class="alternative-emotions">
                                <h4>Other Emotion Probabilities</h4>
                                <div class="emotion-bars" id="emotionBars">
                                    <!-- Dynamically generated -->
                                </div>
                            </div>
                        </div>

                        <!-- Audio Visualization -->
                        <div class="audio-visualization" id="audioVisualization" style="display: none;">
                            <h4>Audio Waveform</h4>
                            <canvas id="waveformCanvas" width="800" height="200"></canvas>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Feature Analysis -->
            <section class="features-section">
                <div class="card">
                    <div class="card__header">
                        <h3>Detailed Feature Analysis</h3>
                    </div>
                    <div class="card__body">
                        <div class="features-grid" id="featuresGrid">
                            <!-- Dynamically generated -->
                        </div>
                        
                        <!-- Spectral Analysis Chart -->
                        <div class="spectral-analysis" id="spectralAnalysis" style="display: none;">
                            <h4>Frequency Spectrum</h4>
                            <canvas id="spectrumCanvas" width="800" height="300"></canvas>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Educational Info -->
            <section class="info-section">
                <div class="card">
                    <div class="card__header">
                        <h3>Analysis Method Explanation</h3>
                    </div>
                    <div class="card__body">
                        <div class="info-grid">
                            <div class="info-item">
                                <h4>MFCC (Mel-Frequency Cepstral Coefficients)</h4>
                                <p>Coefficients representing spectral characteristics of speech, used to distinguish emotions through timbre and pronunciation features.</p>
                            </div>
                            <div class="info-item">
                                <h4>Pitch Variation Analysis</h4>
                                <p>Analyzes patterns of voice pitch changes to measure excitement level and emotional intensity.</p>
                            </div>
                            <div class="info-item">
                                <h4>Speech Rate</h4>
                                <p>Calculates phonemes per unit time to determine urgency, tension level, and energy level.</p>
                            </div>
                            <div class="info-item">
                                <h4>Energy Distribution</h4>
                                <p>Analyzes energy distribution across frequency bands to measure voice intensity and clarity.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Model Info Section -->
            <section class="model-info-section">
                <div class="card">
                    <div class="card__header">
                        <h3>âš ï¸ Current Model Limitations</h3>
                    </div>
                    <div class="card__body">
                        <div class="warning-content">
                            <p><strong>Note:</strong> This demo uses rule-based analysis, not machine learning models. For production use, consider:</p>
                            <ul>
                                <li><strong>HuggingFace Models:</strong> Pre-trained emotion recognition models</li>
                                <li><strong>Google Cloud Speech-to-Text + Sentiment:</strong> Professional-grade analysis</li>
                                <li><strong>Azure Cognitive Services:</strong> Emotion detection API</li>
                                <li><strong>Custom ML Models:</strong> Train on your specific dataset</li>
                            </ul>
                            <p>This version demonstrates audio feature extraction and basic classification logic.</p>
                        </div>
                    </div>
                </div>
            </section>
        </main>

        <!-- Loading Overlay -->
        <div class="loading-overlay" id="loadingOverlay" style="display: none;">
            <div class="loading-content">
                <div class="loading-spinner"></div>
                <p>Analyzing audio...</p>
            </div>
        </div>

        <!-- Error Modal -->
        <div class="modal-overlay" id="errorModal" style="display: none;">
            <div class="modal">
                <div class="modal__header">
                    <h3>Error</h3>
                    <button class="modal__close" id="errorModalClose">&times;</button>
                </div>
                <div class="modal__body">
                    <p id="errorMessage">An error occurred.</p>
                </div>
            </div>
        </div>
    </div>

    <script src="app.js"></script>
</body>
</html>
```

## app.js (Key parts in English)

```javascript
// Advanced Voice Emotion Analyzer
class AdvancedEmotionAnalyzer {
    constructor() {
        this.audioContext = null;
        this.mediaRecorder = null;
        this.recordedChunks = [];
        this.isRecording = false;
        this.recordingStartTime = null;
        this.recordingTimer = null;
        this.currentAudioData = null;
        this.analysisSettings = {
            windowSize: 3.0,
            confidenceThreshold: 0.6,
            sampleRate: 22050,
            hopLength: 512,
            fftSize: 2048
        };
        
        // Emotion categories (English)
        this.emotions = [
            { name: "Happy", color: "#4CAF50", keywords: ["happy", "joy", "excited"], icon: "ðŸ˜Š" },
            { name: "Sad", color: "#2196F3", keywords: ["sad", "melancholy", "down"], icon: "ðŸ˜¢" },
            { name: "Angry", color: "#F44336", keywords: ["angry", "mad", "frustrated"], icon: "ðŸ˜ " },
            { name: "Fear", color: "#9C27B0", keywords: ["fear", "scared", "anxious"], icon: "ðŸ˜°" },
            { name: "Surprise", color: "#FF9800", keywords: ["surprised", "shocked", "amazed"], icon: "ðŸ˜²" },
            { name: "Neutral", color: "#607D8B", keywords: ["neutral", "calm", "normal"], icon: "ðŸ˜" }
        ];
        
        this.init();
    }
    
    init() {
        console.log('Initializing app...');
        if (document.readyState === 'loading') {
            document.addEventListener('DOMContentLoaded', () => {
                this.setupEventListeners();
                this.setupAudioContext();
            });
        } else {
            this.setupEventListeners();
            this.setupAudioContext();
        }
    }
    
    async setupAudioContext() {
        try {
            this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
            if (this.audioContext.state === 'suspended') {
                await this.audioContext.resume();
            }
            console.log('Audio context initialized successfully');
        } catch (error) {
            console.error('Failed to setup audio context:', error);
        }
    }
    
    setupEventListeners() {
        // File upload
        const fileInput = document.getElementById('audioFileInput');
        const uploadArea = document.getElementById('fileUploadArea');
        
        uploadArea.addEventListener('click', (e) => {
            e.preventDefault();
            fileInput.click();
        });
        
        // Drag and drop events
        uploadArea.addEventListener('dragover', (e) => {
            e.preventDefault();
            uploadArea.classList.add('dragover');
        });
        
        uploadArea.addEventListener('dragleave', (e) => {
            e.preventDefault();
            uploadArea.classList.remove('dragover');
        });
        
        uploadArea.addEventListener('drop', (e) => {
            e.preventDefault();
            uploadArea.classList.remove('dragover');
            const files = e.dataTransfer.files;
            if (files.length > 0) {
                this.handleFileUpload(files[0]);
            }
        });
        
        fileInput.addEventListener('change', (e) => {
            if (e.target.files.length > 0) {
                this.handleFileUpload(e.target.files[0]);
            }
        });
        
        // Recording buttons
        const recordButton = document.getElementById('recordButton');
        const stopRecordButton = document.getElementById('stopRecordButton');
        
        if (recordButton) {
            recordButton.addEventListener('click', () => this.startRecording());
        }
        
        if (stopRecordButton) {
            stopRecordButton.addEventListener('click', () => this.stopRecording());
        }
        
        // Error modal
        const errorModalClose = document.getElementById('errorModalClose');
        if (errorModalClose) {
            errorModalClose.addEventListener('click', () => this.hideErrorModal());
        }
        
        console.log('Event listeners setup complete');
    }
    
    async handleFileUpload(file) {
        console.log('Starting file upload:', file.name);
        
        const validTypes = ['audio/mp3', 'audio/wav', 'audio/mpeg', 'audio/mp4', 'audio/m4a'];
        if (!validTypes.includes(file.type)) {
            this.showError('Unsupported file format. Please use MP3, WAV, or M4A files.');
            return;
        }
        
        try {
            this.showLoading();
            
            if (this.audioContext.state === 'suspended') {
                await this.audioContext.resume();
            }
            
            const arrayBuffer = await file.arrayBuffer();
            const audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
            
            console.log('Audio decoded successfully:', {
                duration: audioBuffer.duration,
                channels: audioBuffer.numberOfChannels,
                sampleRate: audioBuffer.sampleRate
            });
            
            await this.analyzeAudio(audioBuffer);
            
        } catch (error) {
            console.error('File processing error:', error);
            this.showError('Error processing file: ' + error.message);
        } finally {
            this.hideLoading();
        }
    }
    
    async startRecording() {
        try {
            console.log('Requesting microphone access...');
            
            const stream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    sampleRate: 44100,
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true
                } 
            });
            
            this.mediaRecorder = new MediaRecorder(stream, {
                mimeType: 'audio/webm;codecs=opus'
            });
            
            this.recordedChunks = [];
            
            this.mediaRecorder.addEventListener('dataavailable', (event) => {
                if (event.data.size > 0) {
                    this.recordedChunks.push(event.data);
                }
            });
            
            this.mediaRecorder.addEventListener('stop', () => {
                console.log('Recording completed');
                this.processRecording();
            });
            
            this.mediaRecorder.start();
            this.isRecording = true;
            this.recordingStartTime = Date.now();
            
            this.updateRecordingUI();
            
            this.recordingTimer = setInterval(() => {
                this.updateRecordingTime();
            }, 1000);
            
            console.log('Recording started');
            
        } catch (error) {
            console.error('Recording start error:', error);
            this.showError('Cannot access microphone. Please check your browser settings and permissions.');
        }
    }
    
    stopRecording() {
        if (this.mediaRecorder && this.isRecording) {
            this.mediaRecorder.stop();
            this.isRecording = false;
            
            if (this.mediaRecorder.stream) {
                this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
            }
            
            if (this.recordingTimer) {
                clearInterval(this.recordingTimer);
                this.recordingTimer = null;
            }
            
            this.updateRecordingUI();
            console.log('Recording stopped');
        }
    }
    
    // ... rest of the methods remain the same with English text updates
    
    classifyEmotion(features) {
        console.log('Starting emotion classification');
        
        // Rule-based classification (NOT ML model - this is the limitation!)
        const scores = {
            Happy: 0,
            Sad: 0,
            Angry: 0,
            Fear: 0,
            Surprise: 0,
            Neutral: 0.3 // Default baseline
        };
        
        // RMS energy based classification
        if (features.rms > 0.1) {
            scores.Happy += 0.3;
            scores.Angry += 0.4;
            scores.Surprise += 0.2;
        } else if (features.rms < 0.05) {
            scores.Sad += 0.4;
            scores.Fear += 0.2;
        }
        
        // Spectral centroid based
        if (features.spectralCentroid > 2000) {
            scores.Happy += 0.2;
            scores.Surprise += 0.3;
            scores.Angry += 0.1;
        } else if (features.spectralCentroid < 1000) {
            scores.Sad += 0.3;
            scores.Fear += 0.2;
        }
        
        // ZCR based
        if (features.zcr > 0.1) {
            scores.Angry += 0.2;
            scores.Fear += 0.1;
        }
        
        // Pitch variation based
        if (features.pitchVariation > 50) {
            scores.Happy += 0.2;
            scores.Surprise += 0.3;
            scores.Angry += 0.1;
        } else if (features.pitchVariation < 20) {
            scores.Sad += 0.2;
            scores.Neutral += 0.1;
        }
        
        // Energy distribution based
        if (features.energyDistribution.variance > 0.01) {
            scores.Angry += 0.2;
            scores.Happy += 0.1;
        }
        
        // Temporal features based
        if (features.temporalFeatures.speechRate > 10) {
            scores.Happy += 0.1;
            scores.Angry += 0.2;
            scores.Surprise += 0.1;
        } else if (features.temporalFeatures.speechRate < 5) {
            scores.Sad += 0.2;
        }
        
        // Normalize
        const total = Object.values(scores).reduce((sum, val) => sum + val, 0);
        if (total > 0) {
            for (const emotion in scores) {
                scores[emotion] /= total;
            }
        }
        
        const primaryEmotion = Object.keys(scores).reduce((a, b) => 
            scores[a] > scores[b] ? a : b
        );
        
        const confidence = scores[primaryEmotion];
        
        return {
            primary: primaryEmotion,
            confidence: confidence,
            scores: scores,
            isConfident: confidence >= this.analysisSettings.confidenceThreshold
        };
    }
    
    showError(message) {
        console.error('Error:', message);
        
        const errorModal = document.getElementById('errorModal');
        const errorMessage = document.getElementById('errorMessage');
        
        if (errorMessage) {
            errorMessage.textContent = message;
        }
        
        if (errorModal) {
            errorModal.style.display = 'flex';
        }
    }
    
    showWarning(message) {
        console.warn('Warning:', message);
        
        const warningDiv = document.createElement('div');
        warningDiv.style.cssText = `
            position: fixed;
            top: 20px;
            right: 20px;
            background: #f59e0b;
            color: white;
            padding: 12px 16px;
            border-radius: 6px;
            z-index: 1001;
            max-width: 300px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        `;
        warningDiv.textContent = message;
        
        document.body.appendChild(warningDiv);
        
        setTimeout(() => {
            if (warningDiv.parentNode) {
                warningDiv.parentNode.removeChild(warningDiv);
            }
        }, 5000);
    }
    
    // ... other methods with English text updates
}

// Initialize app
console.log('Loading app...');
const emotionAnalyzer = new AdvancedEmotionAnalyzer();
console.log('App loaded successfully');
```

## ì‹¤ì œ ë¬´ë£Œ ê³µê°œ ëª¨ë¸ ì‚¬ìš© ë²„ì „ (HuggingFace)

```javascript
// Real ML model integration example
class MLEmotionAnalyzer {
    constructor() {
        this.apiKey = 'YOUR_HUGGINGFACE_TOKEN'; // Get free token from huggingface.co
        this.modelUrl = 'https://api-inference.huggingface.co/models/j-hartmann/emotion-english-distilroberta-base';
    }
    
    async analyzeWithHuggingFace(audioBlob) {
        try {
            // Convert audio to base64
            const base64Audio = await this.blobToBase64(audioBlob);
            
            const response = await fetch(this.modelUrl, {
                headers: {
                    'Authorization': `Bearer ${this.apiKey}`,
                    'Content-Type': 'application/json',
                },
                method: 'POST',
                body: JSON.stringify({
                    inputs: base64Audio
                }),
            });
            
            const result = await response.json();
            return this.processHuggingFaceResult(result);
            
        } catch (error) {
            console.error('HuggingFace API error:', error);
            return this.fallbackToRuleBasedAnalysis();
        }
    }
    
    async blobToBase64(blob) {
        return new Promise((resolve, reject) => {
            const reader = new FileReader();
            reader.onload = () => resolve(reader.result.split(',')[1]);
            reader.onerror = reject;
            reader.readAsDataURL(blob);
        });
    }
    
    processHuggingFaceResult(result) {
        // Process HuggingFace response
        if (result && result.length > 0) {
            const emotions = result.map(item => ({
                emotion: item.label,
                confidence: item.score
            }));
            
            return {
                primary: emotions[0].emotion,
                confidence: emotions[0].confidence,
                scores: emotions,
                isConfident: emotions[0].confidence > 0.7,
                source: 'HuggingFace ML Model'
            };
        }
        
        return this.fallbackToRuleBasedAnalysis();
    }
}

// Google Cloud Speech + Sentiment API example
class GoogleCloudEmotionAnalyzer {
    constructor() {
        this.speechApiKey = 'YOUR_GOOGLE_API_KEY';
        this.sentimentApiKey = 'YOUR_GOOGLE_API_KEY';
    }
    
    async analyzeWithGoogle(audioBlob) {
        try {
            // Step 1: Speech-to-Text
            const transcription = await this.speechToText(audioBlob);
            
            // Step 2: Sentiment Analysis
            const sentiment = await this.analyzeSentiment(transcription);
            
            // Step 3: Combine with audio features
            const audioFeatures = await this.extractAudioFeatures(audioBlob);
            
            return this.combineResults(sentiment, audioFeatures);
            
        } catch (error) {
            console.error('Google Cloud API error:', error);
            return this.fallbackToRuleBasedAnalysis();
        }
    }
}